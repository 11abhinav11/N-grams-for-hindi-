{
 "cells": [
  {
   "cell_type": "raw",
   "id": "89bf923f",
   "metadata": {},
   "source": [
    "We create a simple pipeline function which:\n",
    "splits the datasets by the \\n character\n",
    "remove leading and trailing spaces\n",
    "drop empty sentences.\n",
    "Tokenize sentences using nltk.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c75a16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "## Basic File Paths\n",
    "file_path = \"D:\\program files\\hin_wikipedia_2021_10K\\hin_wikipedia_2021_10K\\hin_wikipedia_2021_10K-sentences.txt\"\n",
    "\n",
    "\n",
    "## Opening the File in read mode (\"r\")\n",
    "with open(file_path,encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "def preprocess_pipeline(data):\n",
    "\n",
    "    # Split by newline character\n",
    "    sentences = np.array(data.split('\\n'))\n",
    "    \n",
    "    # Remove leading and trailing spaces\n",
    "    sentences = np.array([s.strip() for s in sentences])\n",
    "    \n",
    "    # Drop Empty Sentences\n",
    "    sentences = np.array([s for s in sentences if len(s) > 0])\n",
    "    \n",
    "    # Empty List to hold Tokenized Sentences\n",
    "    tokenized = []\n",
    "    \n",
    "    # Iterate through sentences\n",
    "    for sentence in sentences:\n",
    "        # Convert to a list of words\n",
    "        token = word_tokenize(sentence)\n",
    "        for i,tokens in enumerate(token):\n",
    "            if re.search(\"[a-zA-Z0-9]\",tokens):\n",
    "                token.pop(i)\n",
    "        \n",
    "        # Append to list\n",
    "        tokenized.append(token)\n",
    "        \n",
    "    return np.array(tokenized,dtype=object)\n",
    "\n",
    "\n",
    "## Pass our data to this function    \n",
    "tokenized_sentences = preprocess_pipeline(data)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58cc02be",
   "metadata": {},
   "source": [
    "Splitting into Train, Valid and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037b19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain Train and Test Split \n",
    "train, test = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "## Obtain Train and Validation Split \n",
    "train, val = train_test_split(train, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25d41080",
   "metadata": {},
   "source": [
    "As our dataset is quite big, we'll only use those words that appear k times in our dataset. In this function, we'll create a frequency dictionary for our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda3d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_the_words(sentences) -> 'dict':\n",
    "    \n",
    "  # Creating a Dictionary of counts\n",
    "  word_counts = {}\n",
    "\n",
    "  # Iterating over sentences\n",
    "  for sentence in sentences:\n",
    "    \n",
    "    # Iterating over Tokens\n",
    "    for token in sentence:\n",
    "      if re.search(\"[!@#$%&a-zA-Z0-9]\",token) or token in [\"के\",\"में\",\"की\",\"है।\",\"को\",\"का\",\"है\",\"लिए\",\"घाट\"]:\n",
    "        continue\n",
    "      # Add count for new word\n",
    "      if token not in word_counts.keys():\n",
    "        word_counts[token] = 1\n",
    "        \n",
    "      # Increase count by one\n",
    "      else:\n",
    "        word_counts[token] += 1\n",
    "        \n",
    "  return word_counts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "043cd1f7",
   "metadata": {},
   "source": [
    "This function creates a closed vocabulary containing only those words according to the count_threshold parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f213dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_oov(tokenized_sentences, count_threshold) -> 'list':\n",
    "\n",
    "  # Empty list for closed vocabulary\n",
    "  closed_vocabulary = []\n",
    "\n",
    "  # Obtain frequency dictionary using previously defined function\n",
    "  words_count = count_the_words(tokenized_sentences)\n",
    "    \n",
    "  # Iterate over words and counts \n",
    "  for word, count in words_count.items():\n",
    "    \n",
    "    # Append if it's more(or equal) to the threshold \n",
    "    if count >= count_threshold :\n",
    "      closed_vocabulary.append(word)\n",
    "\n",
    "  return closed_vocabulary\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3534399c",
   "metadata": {},
   "source": [
    "In this function we'll add <unk> tokens, to those words which are not in the closed_vocabulary which we just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0a2f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\") -> 'list':\n",
    "\n",
    "  # Convert Vocabulary into a set\n",
    "  vocabulary = set(vocabulary)\n",
    "\n",
    "  # Create empty list for sentences\n",
    "  new_tokenized_sentences = []\n",
    "  \n",
    "  # Iterate over sentences\n",
    "  for sentence in tokenized_sentences:\n",
    "\n",
    "    # Iterate over sentence and add <unk> \n",
    "    # if the token is absent from the vocabulary\n",
    "    new_sentence = []\n",
    "    for token in sentence:\n",
    "      if token in vocabulary:\n",
    "        new_sentence.append(token)\n",
    "      else:\n",
    "        new_sentence.append(unknown_token)\n",
    "    \n",
    "    # Append sentece to the new list\n",
    "    new_tokenized_sentences.append(new_sentence)\n",
    "\n",
    "  return new_tokenized_sentences"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59402160",
   "metadata": {},
   "source": [
    "Final Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2bf641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(train_data, test_data, count_threshold):\n",
    "    \n",
    "  # Get closed Vocabulary\n",
    "  vocabulary = handling_oov(train_data, count_threshold)\n",
    "    \n",
    "  # Updated Training Dataset\n",
    "  new_train_data = unk_tokenize(train_data, vocabulary)\n",
    "    \n",
    "  # Updated Test Dataset\n",
    "  new_test_data = unk_tokenize(test_data, vocabulary)\n",
    "\n",
    "  return new_train_data, new_test_data, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32fa7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 6\n",
    "final_train, final_test, vocabulary = cleansing(train, test, min_freq)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0907b3a6",
   "metadata": {},
   "source": [
    "This function returns a mapping from n-grams to their frequency in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7519965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n",
    "\n",
    "  # Empty dict for n-grams\n",
    "  n_grams = {}\n",
    " \n",
    "  # Iterate over all sentences in the dataset\n",
    "  for sentence in data:\n",
    "        \n",
    "    # Append n start tokens and a single end token to the sentence\n",
    "    sentence = [start_token]*n + sentence + [end_token]\n",
    "    \n",
    "    # Convert the sentence into a tuple\n",
    "    sentence = tuple(sentence)\n",
    "\n",
    "    # Temp var to store length from start of n-gram to end\n",
    "    m = len(sentence) if n==1 else len(sentence)-1\n",
    "    \n",
    "    # Iterate over this length\n",
    "    for i in range(m):\n",
    "        \n",
    "      # Get the n-gram\n",
    "      n_gram = sentence[i:i+n]\n",
    "    \n",
    "      # Add the count of n-gram as value to our dictionary\n",
    "      # IF n-gram is already present\n",
    "      if n_gram in n_grams.keys():\n",
    "        n_grams[n_gram] += 1\n",
    "      # Add n-gram count\n",
    "      else:\n",
    "        n_grams[n_gram] = 1\n",
    "        \n",
    "  return n_grams"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86777ca2",
   "metadata": {},
   "source": [
    "K-smoothing\n",
    "which adds a positive constant k to each numerator and  k×|V| in the denominator, where |V| is the number of words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a618f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n",
    "\n",
    "  # Convert the previous_n_gram into a tuple \n",
    "  previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "  # Calculating the count, if exists from our freq dictionary otherwise zero\n",
    "  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "  \n",
    "  # The Denominator\n",
    "  denom = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "  # previous n-gram plus the current word as a tuple\n",
    "  nplus1_gram = previous_n_gram + (word,)\n",
    "\n",
    "  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero \n",
    "  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
    "\n",
    "  # Numerator\n",
    "  num = nplus1_gram_count + k\n",
    "\n",
    "  # Final Fraction\n",
    "  prob = num / denom\n",
    "  return prob\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "691231b1",
   "metadata": {},
   "source": [
    "Now, we loop over all the words in the vocabulary and then compute their probabilites using our prob_for_single_word() fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d18bec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n",
    "\n",
    "  # Convert to Tuple\n",
    "  previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "  # Add end and unknown tokens to the vocabulary\n",
    "  vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "\n",
    "  # Calculate the size of the vocabulary\n",
    "  vocabulary_size = len(vocabulary)\n",
    "\n",
    "  # Empty dict for probabilites\n",
    "  probabilities = {}\n",
    "\n",
    "  # Iterate over words \n",
    "  for word in vocabulary:\n",
    "    \n",
    "    # Calculate probability\n",
    "    probability = prob_for_single_word(word, previous_n_gram, \n",
    "                                           n_gram_counts, nplus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "    # Create mapping: word -> probability\n",
    "    probabilities[word] = probability\n",
    "\n",
    "  return probabilities"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9eef562",
   "metadata": {},
   "source": [
    "We simply loop over all the words in the vocabulary assuming that they can be the next word and then return the word with it's probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae209083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "\n",
    "    \n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    # most recent 'n' words\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    # Calculate probabilty for all words\n",
    "    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n",
    "\n",
    "    # Intialize the suggestion and max probability\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "\n",
    "    # Iterate over all words and probabilites, returning the max.\n",
    "    # We also add a check if the start_with parameter is provided\n",
    "    for word, prob in probabilities.items():\n",
    "        if word!=\"<unk>\":\n",
    "            if start_with != None: \n",
    "            \n",
    "                if not word.startswith(start_with):\n",
    "                    continue \n",
    "\n",
    "            if prob > max_prob: \n",
    "\n",
    "                suggestion = word\n",
    "                max_prob = prob\n",
    "\n",
    "    return suggestion"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fcb9023b",
   "metadata": {},
   "source": [
    "This function just extends from the previously defined function by taking multiple n-gram counts instead of one. This allows us to take unigram, bigram, .. counts into account as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18b0234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "\n",
    "    # See how many models we have\n",
    "    count = len(n_gram_counts_list)\n",
    "    \n",
    "    # Empty list for suggestions\n",
    "    suggestions = []\n",
    "    \n",
    "    # IMP: Earlier \"-1\"\n",
    "    \n",
    "    # Loop over counts\n",
    "    for i in range(count-1):\n",
    "        \n",
    "        # get n and nplus1 counts\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        nplus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        # get suggestions \n",
    "        suggestion = auto_complete(previous_tokens, n_gram_counts,\n",
    "                                    nplus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        # Append to list\n",
    "        suggestions.append(suggestion)\n",
    "        \n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e814693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    n_model_counts = count_n_grams(final_train, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "491696cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_word(sentence):\n",
    "    previous_tokens = word_tokenize(sentence)\n",
    "    suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "    return suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cf84ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['बाद', 'बाद', 'बाद', 'बाद']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = \"हमसे भरे हुए तलैया देखना\"\n",
    "suggestion = generating_word(sentence)\n",
    "display(suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c50c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word: str,\n",
    "                         previous_n_gram: tuple, \n",
    "                         n_gram_counts: dict,\n",
    "                         n_plus1_gram_counts: dict,\n",
    "                         vocabulary_size: int,\n",
    "                         k: float=1.0) -> float:\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "\n",
    "    Args:\n",
    "       word: next word\n",
    "       previous_n_gram: A sequence of words of length n\n",
    "       n_gram_counts: Dictionary of counts of n-grams\n",
    "       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "       vocabulary_size: number of words in the vocabulary\n",
    "       k: positive constant, smoothing parameter\n",
    "\n",
    "    Returns:\n",
    "       A probability\n",
    "    \"\"\"\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (word,)  \n",
    "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)       \n",
    "    return (n_plus1_gram_count + k)/(previous_n_gram_count + k * vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43380caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence: list,\n",
    "                         n_gram_counts: dict,\n",
    "                         n_plus1_gram_counts: dict,\n",
    "                         vocabulary_size: int,\n",
    "                         k: float=1.0):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of sentences\n",
    "\n",
    "    Args:\n",
    "       sentence: List of strings\n",
    "       n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "       vocabulary_size: number of unique words in the vocabulary\n",
    "       k: Positive smoothing constant\n",
    "\n",
    "    Returns:\n",
    "       Perplexity score\n",
    "    \"\"\"\n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "\n",
    "    # prepend <s> and append <e>\n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "\n",
    "    # Cast the sentence from a list to a tuple\n",
    "    sentence = tuple(sentence)\n",
    "\n",
    "    # length of sentence (after adding <s> and <e> tokens)\n",
    "    N = len(sentence)\n",
    "\n",
    "    # The variable p will hold the product\n",
    "    # that is calculated inside the n-root\n",
    "    # Update this in the code below\n",
    "    product_pi = 1.0\n",
    "\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    # Index t ranges from n to N - 1, inclusive on both ends\n",
    "    for t in range(n, N): # complete this line\n",
    "\n",
    "        # get the n-gram preceding the word at position t\n",
    "        n_gram = sentence[t - n: t]\n",
    "\n",
    "        # get the word at position t\n",
    "        word = sentence[t]\n",
    "\n",
    "        # Estimate the probability of the word given the n-gram\n",
    "        # using the n-gram counts, n-plus1-gram counts,\n",
    "        # vocabulary size, and smoothing constant\n",
    "        probability = estimate_probability(\n",
    "            word=word, previous_n_gram=n_gram,\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            n_gram_counts=n_gram_counts,\n",
    "            n_plus1_gram_counts=n_plus1_gram_counts, k=k)\n",
    "\n",
    "        # Update the product of the probabilities\n",
    "        # This 'product_pi' is a cumulative product \n",
    "        # of the (1/P) factors that are calculated in the loop\n",
    "        product_pi *= 1/probability\n",
    "\n",
    "    # Take the Nth root of the product\n",
    "    perplexity = product_pi**(1/N)\n",
    "\n",
    "    ### END CODE HERE ### \n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea93491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.52378170703766\n"
     ]
    }
   ],
   "source": [
    "print(calculate_perplexity(final_test[90],n_gram_counts_list[0],n_gram_counts_list[1],len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "206fd615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325.9235292052117\n"
     ]
    }
   ],
   "source": [
    "total_perplexity = 0\n",
    "for sentence in final_test:\n",
    "    total_perplexity+=calculate_perplexity(sentence,n_gram_counts_list[2],n_gram_counts_list[3],len(vocabulary))\n",
    "print(total_perplexity/len(final_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da42b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
